##Term document matrix representation of words in simple sentences
library(tm)

#define vector of sentences ("docs")
text<- c("this is the first sentence", 
         "this is a second sentence",
         "the third sentence is here")

#convert sentences into a corpus
corp<- Corpus(VectorSource(text))

#compute term frequency
tdm<- TermDocumentMatrix(corp)
inspect(tdm)

#code for term frequency with messy sentences

text<- c("this is the first         sentence!!", 
         "this is a second Sentence :)",
         "the third sentence, is here", 
         "forth of all sentences")
#convert sentences into a corpus
corp<- Corpus(VectorSource(text))

#compute term frequency
tdm<- TermDocumentMatrix(corp)
inspect(tdm)

#tokenization
corp<- tm_map(corp, stripWhitespace)
corp<- tm_map(corp, removePunctuation)
tdm<- TermDocumentMatrix(corp)
inspect(tdm)

#stopwords
library(SnowballC)
corp<- tm_map(corp, removeWords, stopwords("english"))

#stemming
corp<- tm_map(corp, stemDocument)

tdm<-TermDocumentMatrix(corp)
inspect(tdm)

#TF-IDF Matrix for sentences
tfidf<- weightTfIdf(tdm)
inspect(tfidf)

###Importing and labeling the records, preprocessing text, and producing concept matrix for auto website

#step 1: import and label records
#read zip file into a corpus
corp<- Corpus(ZipSource("AutoAndElectronics.zip", recursive = T))

#create an array of records labels
label <- c(rep(1,1000), rep(0,1000))

#step 2: text preprocessing
#tokenization
corp<- tm_map(corp, stripWhitespace)
corp<- tm_map(corp, removePunctuation)
corp<- tm_map(corp, removeNumbers)

#stopwords
corp<- tm_map(corp, removeWords, stopwords("english"))

#stemming
corp<- tm_map(corp, stemDocument)

#step 3: TF_IDF and latent semantic analysis
#compute TF-IDF
tdm<-TermDocumentMatrix(corp)
tfidf<- weightTfIdf(tdm)

#extract (20) concepts
library(lsa)
lsa.tfidf<- lsa(tfidf)

#convert to data frame
words.df<- as.data.frame(as.matrix(lsa.tfidf$dk))

#fitting a predictive model to the autos and electronics discussion data
#sample 60% of training data
training<- sample(c(1:2000), 0.6*2000)

#run logistic model on training
trainData <- cbind(label = label[training], words.df[training,])
reg<- glm(label ~ ., data = trainData, family = "binomial")

#compute accuracy on validation set
validData <- cbind(label = label[-training], words.df[-training,])
pred<- predict(reg, newdata = validData, type = "response")

#produce confusion matrix
library(caret)
confusionMatrix(as.factor(ifelse(pred>0.5, 1, 0)), as.factor(label[-training]))
